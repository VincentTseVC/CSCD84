CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Gao, Tianxiang

Student Name 2 (last, first): Tse, Vincent

Student number 1: 1001524987

Student number 2: 1001311127

UTORid 1: gaotianx

UTORid 2: tsevinc1

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _Tianxiang Gao__	_Vincent Tse__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
      The rewards function we used consist of the distance to the closest cheese and distance from the closest cat.
      We think it is a good rewards function because the closest cat is the one we would want to prioritize evading and
      the closest cheese would be the one we want to try our best to get, since it's likely to be the easiest to get.

2 .- These are multiple experiments (once you are sure your
     QLearning code is working!)

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 100000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 0.061438

     Train your mouse, and once trained, run the evaluation and
     record the mouse winning rate: 0.586957

     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 0.066701

     Train your mouse, and once trained, run the evaluation and
     record the mouse winning rate: 0.737037

     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?
     The mouse would definitely improve a lot if you trained it with more rounds, but it probably won't be
     invincible since there are initial states where the mouse can't avoid getting eaten unless the cat
     makes a mistake.

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.

     Mouse Winning Rate: 0.444444

     (2 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.

     Mouse Winning Rate: 0.700000

     Average rate for Experiement 3 and Experiment 4: 0.572222

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
     The mouse had a significantly lower success rate, because regular Q-Learning is dependant on which environment
     the QTable is created in.

5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts): 0.029467

     Mouse Winning Rate (from evaluation after training): 0.489382

     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
     Experiment 5 had a significantly lower win rate, probably because there are a lot more
     states to consider so we probably need to increase the number of trials if we want to
     see similiar win rates.

6 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above

     No, Q-Learning is dependent on a static environment. Looking at Experiment 3/4 vs Experiment 2,
     we can see that a QTable generated from a different environment would not work well. If the environment
     is not static, we would need to consider them in our states (which would increase the size of the state
     drastically). Looking at Experiment 5, if we increase the size of the state, we would need to increase the
     amount of trials to train the mouse to have an equivalent winning rate.

7 .- (3 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win
We have 13 features:

Feature 1: 1 if the mouse position is on a cheese location which should be a great state. (0 otherwise)
Feature 2: 1 if the mouse position is on a cat location which should be a bad state. (0 otherwise)
Feature 3: the number of walls around the cat (minus 1). The max number of walls around a cell is 3 so this is
     always in [-1, 1]
Feature 4: 1 if there is a cat very close (within 4 units), signifying a danger state. (0 otherwise)
Feature 5: 1 if there is a cheese very close (within 4 units), signifying a great opportunity. (0 otherwise)
Feature 6-10: all different checks on the shortest path to the closest cheese (Very close, close, very far, etc)
Feature 11: Averages Feature 3, but includes all adjacent and diagonal cells. The more walls around the mouse, the harder it is for the mouse to move around.
Feature 12: The number of cheese around the mouse. If there is alot, then that gives mouse a lot of opportunities.
Feature 12: The number of cats around the mouse. If there is alot, then that gives mouse less opportunities.

8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,
     # random seed = 1522, and use 1000000 trials and 50 rounds.

     Initial mouse winning rate (first rate obtained when training starts): 0.035691

     Mouse Winning Rate (from evaluation after training): 0.596116

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 0.521321

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 0.681818

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes
     to the environment?
     The feature-based Q-Learning does not care about the environment as much as regular
     Q-Learning. We see that in Experiment 8, the winning rate was actually better than
     the winning rate of the environments where the weights are created. Because the features we
     used do not change between different boards, the feature-based Q-Learning has an easier
     time adapting to different environments

9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,
     # random seed = 1522, and use 1000000 trials and 50 rounds.

     Initial mouse winning rate (first rate obtained when training starts): 0.090580

     Mouse Winning Rate (from evaluation after training): 0.652655

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats,
     # 3 cheeses, cat-smartness=.9, and random seed 4289

     Mouse Winning Rate (from evaluation after training): 0.333333

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats,
     # 3 cheeses, cat-smartness=.9, and random seed 4289

     Mouse Winning Rate (from evaluation after training): 0.321521

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs.
     feature-based Q-learning?
     Though Experiment 9 did not do a great job demonstrating the adaptability of feature-based
     Q-Learning, in Experiment 7/8, we see that changes in the environment does not really affect
     this algorithm unlike Experiment 4/5. This means we only need to spend time creating weights from
     one environment and it could be used effectively in every single maze. For regular QLearning,
     we would need to create a QTable for every environment we encounter for it to be effective.

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?

     One great way is to program a simulation for the robot. Because feature-based Q-Learning does not
     care too much about the environment so as long as every big feature we would see in the real world
     exists somewhere in the simulation, the robot can be trained there.



_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.

			Complete/Working	Partial		Not done

QLearn
 update             x

Reward
 function           x

Decide
 action             x

featureEval         x

evaluateQsa         x

maxQsa_prime        x

Qlearn_features     x

decideAction_       x
     features

_____________________________________________________

Marking:

(10 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(20 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(25 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 100


